{
  "name": "create-prompt-and-dataset",
  "description": "Create a new Freeplay project with a prompt template and dataset for testing",
  "user_prompt": "I need to set up Freeplay resources for testing. Please:\n\n1. Create a simple prompt template called \"qa-assistant\" in Freeplay with:\n   - A system message: \"You are a helpful assistant that answers questions.\"\n   - A user message with a {{question}} variable\n   - Model: gpt-4o-mini\n   - Provider: openai\n2. Create a dataset called \"qa-assistant-test-dataset\" for this prompt with at least 3 test cases containing different questions and expected answers\n3. Run a test run using the code in project/run_test.py to execute the prompt against the dataset you created and record completions to Freeplay\n\nIMPORTANT:\n- Do not ask questions - proceed with reasonable defaults\n- Use the Freeplay MCP tools when available\n- Use the project specified in FREEPLAY_PROJECT_ID environment variable\n- The following environment variables are set: FREEPLAY_API_KEY, FREEPLAY_BASE_URL, FREEPLAY_PROJECT_ID, OPENAI_API_KEY\n- Deploy the prompt to the \"dev\" environment\n- Add diverse test questions to the dataset (e.g., factual, math, general knowledge)\n- After creating the prompt and dataset, run the test script: python project/run_test.py",
  "project_dir": "project/",
  "max_turns": 25,
  "success_criteria": [
    {
      "type": "api_verify",
      "method": "check_prompt_exists",
      "prompt_name": "qa-assistant",
      "description": "Prompt template 'qa-assistant' exists"
    },
    {
      "type": "api_verify",
      "method": "check_prompt_has_variable",
      "prompt_name": "qa-assistant",
      "variable_name": "question",
      "description": "Prompt template has 'question' variable"
    },
    {
      "type": "api_verify",
      "method": "check_dataset_exists",
      "prompt_name": "qa-assistant",
      "dataset_name": "qa-assistant-test-dataset",
      "description": "Dataset 'qa-assistant-test-dataset' exists"
    },
    {
      "type": "api_verify",
      "method": "check_dataset_has_test_cases",
      "prompt_name": "qa-assistant",
      "dataset_name": "qa-assistant-test-dataset",
      "min_count": 3,
      "description": "Dataset has at least 3 test cases"
    },
    {
      "type": "api_verify",
      "method": "check_test_run_exists",
      "dataset_name": "qa-assistant-test-dataset",
      "description": "Test run was created for the dataset"
    },
    {
      "type": "api_verify",
      "method": "check_test_run_has_sessions",
      "min_count": 3,
      "description": "Test run executed with at least 3 sessions"
    }
  ],
  "scoring": {
    "prompt_created": {
      "points": 20,
      "description": "Prompt template was created with correct structure"
    },
    "prompt_has_variable": {
      "points": 15,
      "description": "Prompt template uses the question variable"
    },
    "dataset_created": {
      "points": 20,
      "description": "Dataset was created for the prompt"
    },
    "test_cases_added": {
      "points": 15,
      "description": "At least 3 test cases were added to dataset"
    },
    "test_run_created": {
      "points": 15,
      "description": "Test run was successfully created"
    },
    "test_run_executed": {
      "points": 15,
      "description": "Test run executed with completions recorded"
    }
  }
}
